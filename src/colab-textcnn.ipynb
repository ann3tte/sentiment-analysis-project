{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just a cell with all the other cells combined so they don't have to be run individually\n",
    "# Was made using Google Colab\n",
    "\n",
    "# Because I used Google Colab, it can only take\n",
    "# one .csv file per run so this code will have to be re-run everytime\n",
    "# when training with a different.csv file.\n",
    "# File upload code is also Google Colab speficially.\n",
    "# So separately train using Amazon.csv, Tweets.csv, and YouTube.csv\n",
    "# and each time run this code cell.\n",
    "\n",
    "# To install required libraries, uncomment the line below:\n",
    "# !pip install tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "  uploaded = files.upload()\n",
    "  filename = next(iter(uploaded))\n",
    "  df = pd.read_csv(filename)\n",
    "\n",
    "  LABEL_MAP = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "  df.dropna(subset=['text'], inplace=True)\n",
    "  df['sentiment'] = df['sentiment'].map(LABEL_MAP)\n",
    "  df['text'] = df['text'].apply(preprocess_text)\n",
    "  texts = df['text'].tolist()\n",
    "  labels = df['sentiment'].astype(int).tolist()\n",
    "\n",
    "  return texts, labels, LABEL_MAP\n",
    "\n",
    "\n",
    "def tokenize_text(texts, max_features, max_length):\n",
    "\n",
    "  tokenizer = Tokenizer(num_words=max_features)\n",
    "  tokenizer.fit_on_texts(texts)\n",
    "  sequences = tokenizer.texts_to_sequences(texts)\n",
    "  X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "  return X\n",
    "\n",
    "\n",
    "def CNN_model(max_features, embedding_dim, max_length, num_filters, kernel_size, hidden_dim):\n",
    "  CNN_model = models.Sequential()\n",
    "\n",
    "  CNN_model.add(layers.Embedding(max_features, embedding_dim, input_length=max_length))\n",
    "  CNN_model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "  CNN_model.add(layers.GlobalMaxPooling1D())\n",
    "  CNN_model.add(layers.Dense(hidden_dim, activation='relu'))\n",
    "  CNN_model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "  CNN_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return CNN_model\n",
    "\n",
    "\n",
    "max_features = 1000\n",
    "max_length = 200\n",
    "embedding_dim = 100\n",
    "num_filters = 128\n",
    "kernel_size = 3\n",
    "hidden_dim = 128\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Preprocess data\n",
    "texts, labels, LABEL_MAP = preprocess_data()\n",
    "\n",
    "X = tokenize_text(texts, max_features, max_length)\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train CNN model\n",
    "model = CNN_model(max_features, embedding_dim, max_length, num_filters, kernel_size, hidden_dim)\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=LABEL_MAP.keys()))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To install required libraries, uncomment the line below:\n",
    "# !pip install tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from google.colab import files"
   ],
   "id": "7f5f98c0cfbfd282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip().lower()"
   ],
   "id": "fff4fa6f040e9a8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data():\n",
    "  uploaded = files.upload()\n",
    "  filename = next(iter(uploaded))\n",
    "  df = pd.read_csv(filename)\n",
    "\n",
    "  LABEL_MAP = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "  df.dropna(subset=['text'], inplace=True)\n",
    "  df['sentiment'] = df['sentiment'].map(LABEL_MAP)\n",
    "  df['text'] = df['text'].apply(preprocess_text)\n",
    "  texts = df['text'].tolist()\n",
    "  labels = df['sentiment'].astype(int).tolist()\n",
    "\n",
    "  return texts, labels, LABEL_MAP"
   ],
   "id": "e98545ea3632dcd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_text(texts, max_features, max_length):\n",
    "\n",
    "  tokenizer = Tokenizer(num_words=max_features)\n",
    "  tokenizer.fit_on_texts(texts)\n",
    "  sequences = tokenizer.texts_to_sequences(texts)\n",
    "  X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "  return X"
   ],
   "id": "bdc9eb49b7886cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def CNN_model(max_features, embedding_dim, max_length, num_filters, kernel_size, hidden_dim):\n",
    "  CNN_model = models.Sequential()\n",
    "\n",
    "  CNN_model.add(layers.Embedding(max_features, embedding_dim, input_length=max_length))\n",
    "  CNN_model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "  CNN_model.add(layers.GlobalMaxPooling1D())\n",
    "  CNN_model.add(layers.Dense(hidden_dim, activation='relu'))\n",
    "  CNN_model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "  CNN_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return CNN_model"
   ],
   "id": "7a81b24492e7d852"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Because I used Google Colab, it can only take\n",
    "# one .csv file per run so this code will have to be re-run everytime\n",
    "# when training with a different.csv file.\n",
    "# So separately train using Amazon.csv, Tweets.csv, and YouTube.csv\n",
    "# and each time run this code cell.\n",
    "\n",
    "\n",
    "max_features = 1000\n",
    "max_length = 200\n",
    "embedding_dim = 100\n",
    "num_filters = 128\n",
    "kernel_size = 3\n",
    "hidden_dim = 128\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Preprocess data\n",
    "texts, labels, LABEL_MAP = preprocess_data()\n",
    "\n",
    "X = tokenize_text(texts, max_features, max_length)\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train CNN model\n",
    "model = CNN_model(max_features, embedding_dim, max_length, num_filters, kernel_size, hidden_dim)\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=LABEL_MAP.keys()))"
   ],
   "id": "5c79facb2979b7b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
